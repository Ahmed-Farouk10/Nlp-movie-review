{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42881d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa45e3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sampleSubmission.csv', 'test.tsv.zip', 'train.tsv.zip']\n",
      "['train.tsv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "zip_file_path = 'sentiment-analysis-on-movie-reviews.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as outer_zip_ref:\n",
    "    # List the files in the outer ZIP archive\n",
    "    print(outer_zip_ref.namelist())  # This will print the list of files in the outer ZIP archive\n",
    "    \n",
    "    # Extract the 'train.tsv.zip' file\n",
    "    outer_zip_ref.extract('train.tsv.zip')\n",
    "    \n",
    "    # Open the inner ZIP file ('train.tsv.zip')\n",
    "    with zipfile.ZipFile('train.tsv.zip', 'r') as inner_zip_ref:\n",
    "        # List the files in the inner ZIP archive\n",
    "        print(inner_zip_ref.namelist())  # This will print the list of files in the inner ZIP archive\n",
    "        \n",
    "        # Read the 'train.tsv' file from the inner ZIP archive\n",
    "        with inner_zip_ref.open('train.tsv') as file:\n",
    "            train_data = pd.read_csv(file, sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18c3a62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PhraseId', 'SentenceId', 'Phrase', 'Sentiment'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data.columns)\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c28423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed43917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning\n",
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f348d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and lowercasing\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13d5aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c386b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    preprocessed_data = []\n",
    "    for text in data:\n",
    "        text = clean_text(text)\n",
    "        tokens = tokenize_text(text)\n",
    "        tokens = remove_stopwords(tokens)\n",
    "        preprocessed_data.append(tokens)\n",
    "    return preprocessed_data\n",
    "\n",
    "# Preprocess the 'Phrase' column in the DataFrame\n",
    "train_data['Preprocessed_Phrase'] = preprocess_data(train_data['Phrase'])\n",
    "\n",
    "# Convert words to integers using Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['Preprocessed_Phrase'])\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['Preprocessed_Phrase'])\n",
    "\n",
    "# Padding sequences\n",
    "max_sequence_length = max([len(seq) for seq in train_sequences])\n",
    "padded_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "406572e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # Import train_test_split here\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, train_data['Sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4691dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128),\n",
    "    tf.keras.layers.SimpleRNN(units=64),\n",
    "    tf.keras.layers.Dense(units=5, activation='softmax')  # 5 classes for sentiment\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2577173b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - accuracy: 0.5333 - loss: 1.1808 - val_accuracy: 0.6179 - val_loss: 0.9567\n",
      "Epoch 2/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.6495 - loss: 0.8751 - val_accuracy: 0.6141 - val_loss: 0.9366\n",
      "Epoch 3/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.6796 - loss: 0.7904 - val_accuracy: 0.6471 - val_loss: 0.8779\n",
      "Epoch 4/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.7034 - loss: 0.7325 - val_accuracy: 0.6509 - val_loss: 0.8818\n",
      "Epoch 5/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.7176 - loss: 0.6984 - val_accuracy: 0.6532 - val_loss: 0.8721\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3d2a447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 22ms/step - accuracy: 0.5270 - loss: 1.2093 - val_accuracy: 0.6152 - val_loss: 0.9682\n",
      "Epoch 2/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 23ms/step - accuracy: 0.6490 - loss: 0.8816 - val_accuracy: 0.6578 - val_loss: 0.8471\n",
      "Epoch 3/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.6978 - loss: 0.7507 - val_accuracy: 0.6639 - val_loss: 0.8289\n",
      "Epoch 4/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.7203 - loss: 0.6924 - val_accuracy: 0.6679 - val_loss: 0.8331\n",
      "Epoch 5/5\n",
      "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 23ms/step - accuracy: 0.7326 - loss: 0.6532 - val_accuracy: 0.6665 - val_loss: 0.8641\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128),\n",
    "    tf.keras.layers.LSTM(units=64),\n",
    "    tf.keras.layers.Dense(units=5, activation='softmax')  # 5 classes for sentiment\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63479a39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
